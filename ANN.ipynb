{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3099ad8e-9228-4bed-96f2-6ebce88d2d93",
   "metadata": {},
   "source": [
    "# **Approximate Nearest Neighbour Search**\n",
    "\n",
    "* Local Sensivity Hashing Using FAISS \n",
    "* Exhaustive Search using FAISS\n",
    "* Product Quantization using FAISS\n",
    "* Trees And Graphs using Annoy\n",
    "* HNSW using NSMLIB\n",
    "\n",
    "### **Dynamic score calculation of MF algorithm**\n",
    "Matrix Factorization (MF) algorithms are becoming more popular these days. In MF, the recommendation score is calculated as the inner product of the generated vector of user items, but if the number of users and items is large, the data size to be retained is very large if the score is pre-calculated for all user x item combinations. It will be.\n",
    "\n",
    "### **High speed vector calculation library faiss**\n",
    "Faiss, developed by Facebook Research, is a library that can perform such vector calculations at high speed. It can index a set of vectors (matrix) in the memory in advance and execute score calculation and sorting instantly for the vector given as input.\n",
    "\n",
    "faiss has the following characteristics:\n",
    "\n",
    "* GPU and multithreaded support for index operations\n",
    "* Dimensionality reduction: vectors with large dimensions can be reduced to smaller dimensions using PCA\n",
    "* Reduction of spatial complexity by dimensional compression\n",
    "* Quantisation: FAISS emphasises on product quantisation for compressing and storing vectors of large dimensions\n",
    "* Batch processing i.e searching for multiple queries at a time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "e63c107f-845e-4c2a-b245-6f147b956195",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip3 install lightfm\n",
    "# !pip3.6 install annoy\n",
    "# !pip3 install nmslib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "f7dcea75-f35f-451b-8b76-bb00b890863b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import faiss\n",
    "import numpy\n",
    "from scipy.sparse import coo_matrix\n",
    "from sklearn.decomposition import NMF\n",
    "from annoy import AnnoyIndex"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56646325-7626-4e1c-9aee-fa12216e6f27",
   "metadata": {},
   "outputs": [],
   "source": [
    "# constants\n",
    "RANDOM_STATE = 0\n",
    "N_FACTOR = 20\n",
    "N_RESULT = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "e3841ef2-bf26-41a2-84d4-1a269994b558",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load dataset\n",
    "ratings = numpy.loadtxt(\n",
    "    'ratings.csv',\n",
    "    delimiter=',',\n",
    "    skiprows=1,\n",
    "    usecols=(0, 1, 2),\n",
    "    dtype=[('userId', 'i8'), ('movieId', 'i8'), ('rating', 'f8')],\n",
    ")\n",
    "\n",
    "# movies = numpy.loadtxt(\n",
    "#     '../Datasets/movielens-small/movies.csv',\n",
    "#     delimiter=',',\n",
    "#     skiprows=1,\n",
    "#     usecols=(0, 1, 2),\n",
    "#     dtype=[('movieId', 'i8'), ('title', '|S15'), ('genres', '|S15')],\n",
    "# )\n",
    "\n",
    "# movies = numpy.loadtxt('../Datasets/movielens-small/movies.csv',\n",
    "#    dtype={'names': ('movieId', 'title', 'genre'),\n",
    "#           'formats': ('i8', '|S15', '|S15')}, delimiter=',', skiprows=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "b963cac2-6088-41dd-98b9-0449b4280c9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "users = sorted(numpy.unique(ratings['userId']))\n",
    "movies = sorted(numpy.unique(ratings['movieId']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "7c2f4bf4-6362-4682-8e0c-534b0c4043e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for later use\n",
    "user_id2i = {id: i for i, id in enumerate(users)}\n",
    "movie_id2i = {id: i for i, id in enumerate(movies)}\n",
    "movie_i2id = {i: id for i, id in enumerate(movies)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "30d31b24-7c1c-469f-9b3a-de0e15b94b31",
   "metadata": {},
   "outputs": [],
   "source": [
    "mapped_user = numpy.array(list(map(user_id2i.get, ratings['userId'])))\n",
    "mapped_movieId = numpy.array(list(map(movie_id2i.get, ratings['movieId'])))\n",
    "                          \n",
    "# make sparse matrix\n",
    "rating_mat = coo_matrix(\n",
    "    (ratings['rating'], (list(map(user_id2i.get, ratings['userId'])),\n",
    "                         list(map(movie_id2i.get, ratings['movieId']))))\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "40304ac7-2f3c-456c-b06b-53f336ee44a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/nktech/Desktop/preeti/Fall-2021/CMPE257-Machine-Learning/venv36/lib/python3.6/site-packages/sklearn/decomposition/_nmf.py:1091: ConvergenceWarning: Maximum number of iterations 200 reached. Increase it to improve convergence.\n",
      "  \" improve convergence.\" % max_iter, ConvergenceWarning)\n"
     ]
    }
   ],
   "source": [
    "# decompose\n",
    "model = NMF(n_components=N_FACTOR, init='random', random_state=RANDOM_STATE)\n",
    "user_mat = model.fit_transform(rating_mat)\n",
    "movie_mat = model.components_.T"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90aa0d2a-8e8d-4aa7-b830-8b260c96432d",
   "metadata": {},
   "source": [
    "### **Exhaustive Search**\n",
    "IndexFlatL2 measures the L2 (or Euclidean) distance between all given points between our query vector, and the vectors loaded into the index. Itâ€™s simple, very accurate, but not too fast."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "8e289f9e-adbc-4da6-bdb8-29f9f0b67585",
   "metadata": {},
   "outputs": [],
   "source": [
    "# indexing\n",
    "movie_index = faiss.IndexFlatL2(N_FACTOR)\n",
    "movie_index.add(movie_mat.astype('float32'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "06b34b00-cf44-45eb-80cb-90634a35a775",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Query n vectors of dimension d to the index.\n",
    "Return at most k vectors. If there are not enough results for a query, the result array is padded with -1s.\n",
    "\"\"\"\n",
    "\n",
    "def search(user_id, index, n=N_RESULT):\n",
    "    user_i = user_id2i[user_id]\n",
    "    user_vec = user_mat[user_i].astype('float32')\n",
    "    \n",
    "    scores, indices = index.search(numpy.array([user_vec]), n)\n",
    "    movie_scores = zip(indices[0], scores[0])\n",
    "\n",
    "    movies=[\n",
    "        {\n",
    "            \"id\": int(movie_i2id[i]),\n",
    "            \"score\": float(s),\n",
    "        }\n",
    "        for i, s in movie_scores\n",
    "    ]\n",
    "    return movies\n",
    "    # return [name[i] for i in indices[0]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "64ef5c44-2430-42a4-bf97-721c70233996",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'id': 1796, 'score': 0.0030687162652611732},\n",
       " {'id': 6185, 'score': 0.0030687162652611732},\n",
       " {'id': 1861, 'score': 0.0030687162652611732},\n",
       " {'id': 5256, 'score': 0.0030687162652611732},\n",
       " {'id': 137, 'score': 0.003425348550081253},\n",
       " {'id': 1640, 'score': 0.003425348550081253},\n",
       " {'id': 1325, 'score': 0.003425348550081253},\n",
       " {'id': 1731, 'score': 0.003425348550081253},\n",
       " {'id': 526, 'score': 0.003425348550081253},\n",
       " {'id': 3820, 'score': 0.003425348550081253}]"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "search(35, movie_index)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7ae46c8-717b-4fee-b2d3-d30b154db689",
   "metadata": {},
   "source": [
    "### **Local Sensitivity Hashing**\n",
    "An algorithmic technique that hashes similar input items into the same \"buckets\" with high probability.(The number of buckets is much smaller than the universe of possible input items.) Since similar items end up in the same buckets, this technique can be used for data clustering and nearest neighbor search."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "27452130-ffd3-4a40-ae92-58c02c8e32a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# indexing\n",
    "movie_index_lsh = faiss.IndexLSH(movie_mat.shape[1], 8)\n",
    "movie_index_lsh.add(movie_mat.astype('float32'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "57ce5e22-7c27-4d84-a40f-b2acbc5d3943",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'id': 3547, 'score': 0.0},\n",
       " {'id': 3729, 'score': 0.0},\n",
       " {'id': 55768, 'score': 0.0},\n",
       " {'id': 2553, 'score': 0.0},\n",
       " {'id': 2844, 'score': 0.0},\n",
       " {'id': 2325, 'score': 0.0},\n",
       " {'id': 1180, 'score': 1.0},\n",
       " {'id': 1128, 'score': 1.0}]"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "search(12, movie_index_lsh, 8) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fc4dad2-9cf9-4822-bdf2-b7e72fcde724",
   "metadata": {},
   "source": [
    "LightFM is a Python implementation of a number of popular recommendation algorithms for both implicit and explicit feedback, including efficient implementation of BPR and WARP ranking losses. It's easy to use, fast (via multithreaded model estimation), and produces high quality results.\n",
    "\n",
    "It also makes it possible to incorporate both item and user metadata into the traditional matrix factorization algorithms. It represents each user and item as the sum of the latent representations of their features, thus allowing recommendations to generalise to new items (via item features) and to new users (via user features)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "e220ad2d-fc79-4b8e-a58a-786afb7769fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "from lightfm import LightFM\n",
    "from lightfm.datasets import fetch_movielens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "4be251b8-539d-4c8b-bc2d-66e5bb13e9de",
   "metadata": {},
   "outputs": [],
   "source": [
    "movielens = fetch_movielens()\n",
    "train = movielens['train']\n",
    "test = movielens['test']\n",
    "\n",
    "model = LightFM(learning_rate=0.05, loss='warp', no_components=64, item_alpha=0.001)\n",
    "model.fit_partial(train, item_features=movielens['item_features'], epochs=20 )\n",
    "\n",
    "vector = movielens['item_features'] * model.item_embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "55058d25-91df-40fe-8784-92f9192ebb62",
   "metadata": {},
   "outputs": [],
   "source": [
    "name = movielens['item_labels']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c3d9b27-79a0-4462-8d55-410299325a9f",
   "metadata": {},
   "source": [
    "### **Trees and Graphs - AnnoyIndex**\n",
    "#### **How does it work**\n",
    "Using random projections and by building up a tree. At every intermediate node in the tree, a random hyperplane is chosen, which divides the space into two subspaces. This hyperplane is chosen by sampling two points from the subset and taking the hyperplane equidistant from them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "831cab25-b300-4c52-941b-30b491b5e90f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/nktech/Desktop/preeti/Fall-2021/CMPE257-Machine-Learning/venv36/lib/python3.6/site-packages/ipykernel_launcher.py:3: FutureWarning: The default argument for metric will be removed in future version of Annoy. Please pass metric='angular' explicitly.\n",
      "  This is separate from the ipykernel package so we can avoid doing imports until\n"
     ]
    }
   ],
   "source": [
    "from annoy import AnnoyIndex\n",
    "x = numpy.array(list(map(numpy.float, ratings['rating'])))\n",
    "annoy_index = AnnoyIndex(vector.shape[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "d6d7e6e1-2d76-4991-86e9-7f73a7ecbb2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "for idx, vec in enumerate(vector):\n",
    "    annoy_index.add_item(idx, vec.tolist())\n",
    "\n",
    "search_in_x_trees = annoy_index.build(10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "09ddaa87-bcc1-4f0d-b2fa-111fc215ae4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "result = annoy_index.get_nns_by_vector(vector[0].tolist(), 10, search_k=search_in_x_trees)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "b1b5f919-8462-4461-bf75-07f2551752a1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Toy Story (1995)',\n",
       " 'Star Wars (1977)',\n",
       " 'Return of the Jedi (1983)',\n",
       " 'Star Trek: First Contact (1996)',\n",
       " 'Twelve Monkeys (1995)',\n",
       " 'Mars Attacks! (1996)',\n",
       " 'Willy Wonka and the Chocolate Factory (1971)',\n",
       " 'Birdcage, The (1996)',\n",
       " 'Dragonheart (1996)',\n",
       " 'Men in Black (1997)']"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "to_lables = [name[i] for i in result]\n",
    "to_lables"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62e1fc33-a3d8-497f-bf77-ea1f0c0e8c07",
   "metadata": {},
   "source": [
    "### **Hierarchical NSW**\n",
    "Hierarchical NSW incrementally builds a multi-layer structure consisting from hierarchical set of proximity graphs (layers) for nested subsets of the stored elements. The maximum layer in which an element is present is selected randomly with an exponentially decaying probability distribution. This allows producing graphs similar to the previously studied Navigable Small World (NSW) structures while additionally having the links separated by their characteristic distance scales. Starting search from the upper layer together with utilizing the scale separation boosts the performance compared to NSW and allows a logarithmic complexity scaling.\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "ed464745-0b79-46a7-a85f-5654fa0c80a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nmslib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "a222735c-71d4-43a8-b120-1c990d0a4102",
   "metadata": {},
   "outputs": [],
   "source": [
    "nms_index = nmslib.init(method='hnsw', space='cosinesimil')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "8c05b709-0f39-44f9-9a9a-236e6ac9ca23",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1682"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nms_index.addDataPointBatch(vector)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "b06c6d75-3c19-436a-b647-6dee9d0852ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "nms_index.createIndex({'post': 2})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "0dc1f4ab-b2a2-4fdd-87ec-f118eacb18c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "nms_result = nms_index.knnQuery(vector, k=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "4462c536-e995-4608-a99f-07a1cffb31b6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([1190, 1589, 1617, 1643, 1651, 1661, 1664, 1665, 1666, 1669],\n",
       "       dtype=int32),\n",
       " array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.], dtype=float32))"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nms_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "9d281448-88f0-410c-b115-78a03e92eeeb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Letter From Death Row, A (1998)',\n",
       " 'To Have, or Not (1995)',\n",
       " 'King of New York (1990)',\n",
       " 'Sudden Manhattan (1996)',\n",
       " 'Temptress Moon (Feng Yue) (1996)',\n",
       " 'Rough Magic (1995)',\n",
       " \"Brother's Kiss, A (1997)\",\n",
       " 'Ripe (1996)',\n",
       " 'Next Step, The (1995)',\n",
       " 'Tainted (1998)']"
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def to_movie_name(arr):\n",
    "    return [name[i] for i in arr.tolist()]\n",
    "\n",
    "to_movie_name(nms_result[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17096ad0-6a2f-41a8-a03f-71911880538e",
   "metadata": {},
   "source": [
    "### **Product Quantization**\n",
    "Product quantization is a great way to compress your large data set vectors to enable faster search results. However, this also affects the accuracy of the results returned. Hence the trade-off between accuracy and memory constraints(or speed) should be critically analyzed while choosing any index in FAISS."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "4dd6fe43-7781-47ca-a33b-db7b9aad8629",
   "metadata": {},
   "outputs": [],
   "source": [
    "quantizer = faiss.IndexFlatL2(vector.shape[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "4dc4c9b3-699f-4989-9449-b1dbd4346649",
   "metadata": {},
   "outputs": [],
   "source": [
    "number_of_partition=8\n",
    "search_in_x_partitions=2\n",
    "subvector_size=8\n",
    "\n",
    "prod_quant_index = faiss.IndexIVFPQ(quantizer, vector.shape[1], number_of_partition, search_in_x_partitions, subvector_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "bde99870-8ba8-4348-a363-267cc447345c",
   "metadata": {},
   "outputs": [],
   "source": [
    "prod_quant_index.train(vector)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "424ba36a-515a-4c49-8c89-a997d97e047b",
   "metadata": {},
   "outputs": [],
   "source": [
    "prod_quant_index.add(vector)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "770e037b-8fdd-4906-9cd2-59ec476fed97",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array(['Toy Story (1995)', 'Twelve Monkeys (1995)', 'Phenomenon (1996)',\n",
       "        'Twister (1996)', 'Willy Wonka and the Chocolate Factory (1971)',\n",
       "        'Mars Attacks! (1996)', 'Independence Day (ID4) (1996)',\n",
       "        \"Jackie Chan's First Strike (1996)\", 'Dragonheart (1996)',\n",
       "        'Star Trek: First Contact (1996)'], dtype=object)]"
      ]
     },
     "execution_count": 118,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result = prod_quant_index.search(vector, 10)[1]\n",
    "to_movie_name(result[0:1])\n",
    "# result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2fbb003-6ceb-4433-97e8-4bcfc0b96b51",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "615e7c13-e5d3-4ce9-bb28-5c49d49bed71",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.6",
   "language": "python",
   "name": "python3.6"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
